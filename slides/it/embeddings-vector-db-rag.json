{
  "number": 7,
  "title": "Embeddings, Vector DB e RAG",
  "description": "Dalla ricerca semantica alla Retrieval Augmented Generation",
  "steps": [
    {
      "name": "Introduzione",
      "slides": [
        0,
        1
      ]
    },
    {
      "name": "Embeddings: Teoria",
      "slides": [
        2,
        3,
        4
      ]
    },
    {
      "name": "Embeddings: Pratica",
      "slides": [
        5,
        6,
        7
      ]
    },
    {
      "name": "Vector Databases",
      "slides": [
        8,
        9,
        10
      ]
    },
    {
      "name": "Vector DB Comparison",
      "slides": [
        11,
        12,
        13
      ]
    },
    {
      "name": "RAG Fundamentals",
      "slides": [
        14,
        15,
        16
      ]
    },
    {
      "name": "RAG Solutions",
      "slides": [
        17,
        18,
        19,
        20
      ]
    },
    {
      "name": "Costi e Production",
      "slides": [
        21,
        22,
        23
      ]
    }
  ],
  "slides": [
    {
      "type": "title",
      "title": "Embeddings, Vector DB e RAG",
      "subtitle": "La Ricerca Semantica che Funziona",
      "description": "Come trasformare testo in numeri che <strong>capiscono il significato</strong>",
      "ironicClosing": "Quando cercare \"cane\" trova anche \"cucciolo\" senza keyword stuffing"
    },
    {
      "type": "text",
      "title": "Il Problema della Ricerca Tradizionale",
      "paragraphs": [
        "<strong>Keyword search:</strong> \"python programming\" trova solo \"python programming\"",
        "<strong>Problemi:</strong> Sinonimi ignorati, contesto perso, typo = disaster",
        "",
        "<strong>Semantic search:</strong> Capisce il <em>significato</em>, non solo le parole",
        "Query: \"linguaggio per data science\" → trova documenti su Python, R, Julia"
      ],
      "ironicClosing": "Ctrl+F è del '90. Embeddings sono del 2025."
    },
    {
      "type": "text",
      "title": "Embeddings: La Teoria",
      "paragraphs": [
        "<strong>Definizione:</strong> Rappresentazione numerica (vettore) del significato di un testo",
        "",
        "<strong>Come funziona:</strong>",
        "Testo → Modello transformer → Vettore di numeri (es. 1536 dimensioni)",
        "",
        "<strong>Proprietà magica:</strong> Testi simili = vettori vicini nello spazio",
        "\"cane\" e \"cucciolo\" hanno embedding simili"
      ],
      "ironicClosing": "Trasformare parole in numeri che capiscono il contesto. Magia? No, transformer."
    },
    {
      "type": "text",
      "title": "Embeddings: Spazio Vettoriale",
      "paragraphs": [
        "<strong>Ogni parola/frase diventa un punto in uno spazio N-dimensionale</strong>",
        "",
        "Dimensioni tipiche: 384 (piccolo), 768 (medio), 1536 (grande), 3072 (XL)",
        "",
        "<strong>Relazioni geometriche = Relazioni semantiche:</strong>",
        "• Distanza piccola → Significato simile",
        "• Direzione → Relazione concettuale",
        "• Clustering → Temi correlati"
      ],
      "ironicClosing": "1536 dimensioni. La tua mente non le visualizza, ma l'algebra lineare sì."
    },
    {
      "type": "text",
      "title": "Similarity Metrics: Cosine Similarity",
      "paragraphs": [
        "<strong>Come si misura la somiglianza tra embedding?</strong>",
        "",
        "<strong>Cosine Similarity:</strong> Misura l'angolo tra due vettori",
        "• Range: -1 (opposti) a +1 (identici)",
        "• 0 = ortogonali (non correlati)",
        "",
        "<strong>Altre metriche:</strong> Euclidean distance, Dot product, Manhattan distance",
        "",
        "<strong>Regola d'oro:</strong> Usa la metrica con cui è stato trainato il modello"
      ],
      "ironicClosing": "Cosine similarity: quando la geometria del liceo serve davvero",
      "citations": [
        {
          "text": "Cosine similarity measures angle between vectors, range -1 to +1",
          "source": "Pinecone - Vector Similarity",
          "url": "https://www.pinecone.io/learn/vector-similarity/"
        }
      ]
    },
    {
      "type": "data",
      "title": "Modelli di Embedding: Confronto 2025",
      "intro": "<strong>Chi genera i migliori embeddings?</strong>",
      "metrics": [
        {
          "value": "$0.02/1M",
          "label": "OpenAI text-embedding-3-small (1536 dim)"
        },
        {
          "value": "$0.13/1M",
          "label": "OpenAI text-embedding-3-large (3072 dim)"
        },
        {
          "value": "$0.50/1M",
          "label": "Cohere embed-english-v3.0 (1024 dim)"
        },
        {
          "value": "Free",
          "label": "Sentence Transformers (self-hosted)"
        }
      ],
      "ironicClosing": "OpenAI domina, Cohere costa meno per volume, open source gratis ma serve GPU",
      "citations": [
        {
          "text": "OpenAI text-embedding-3-small $0.02/1M, large $0.13/1M tokens",
          "source": "OpenAI Pricing 2025",
          "url": "https://openai.com/api/pricing/"
        },
        {
          "text": "Cohere embed-english-v3.0 costs $0.50 per 1M tokens",
          "source": "Document360 - Embedding Models",
          "url": "https://document360.com/blog/text-embedding-model-analysis/"
        }
      ]
    },
    {
      "type": "code",
      "title": "Embeddings in Pratica: OpenAI",
      "code": {
        "language": "python",
        "snippet": "from openai import OpenAI\nimport numpy as np\nfrom numpy.linalg import norm\n\nclient = OpenAI()\n\n# Generate embeddings\ntext1 = \"Python is great for data science\"\ntext2 = \"Data analysis with Python is powerful\"\ntext3 = \"I love pizza\"\n\nresponse1 = client.embeddings.create(\n    model=\"text-embedding-3-small\",\n    input=text1\n)\nresponse2 = client.embeddings.create(\n    model=\"text-embedding-3-small\",\n    input=text2\n)\nresponse3 = client.embeddings.create(\n    model=\"text-embedding-3-small\",\n    input=text3\n)\n\nemb1 = np.array(response1.data[0].embedding)\nemb2 = np.array(response2.data[0].embedding)\nemb3 = np.array(response3.data[0].embedding)\n\n# Cosine similarity\ndef cosine_sim(a, b):\n    return np.dot(a, b) / (norm(a) * norm(b))\n\nprint(f\"Similarity 1-2: {cosine_sim(emb1, emb2):.3f}\")  # ~0.85\nprint(f\"Similarity 1-3: {cosine_sim(emb1, emb3):.3f}\")  # ~0.15"
      },
      "explanation": "Testi simili → embedding simili (alta cosine similarity)"
    },
    {
      "type": "text",
      "title": "Embedding Models: Dimensioni e Trade-off",
      "paragraphs": [
        "<strong>Dimensioni:</strong> Più grande = più accurato, ma più costoso",
        "",
        "<strong>OpenAI text-embedding-3-small (1536 dim):</strong> Ottimo rapporto qualità/prezzo",
        "<strong>OpenAI text-embedding-3-large (3072 dim):</strong> Massima accuracy, 8x storage cost",
        "<strong>Cohere embed-v4 (1024 dim):</strong> Multimodal (text + image), efficiente",
        "",
        "<strong>Trade-off:</strong> Accuracy vs Storage vs Costo query"
      ],
      "ironicClosing": "3072 dimensioni per cercare \"how to boil pasta\"? Overkill.",
      "citations": [
        {
          "text": "Storing OpenAI text-embedding-3-large costs 8x more than Cohere V3 light",
          "source": "Elephas - Best Embedding Models 2025",
          "url": "https://elephas.app/blog/best-embedding-models"
        }
      ]
    },
    {
      "type": "text",
      "title": "Vector Database: Perché Servono",
      "paragraphs": [
        "<strong>Problema:</strong> Hai 10 milioni di documenti con embeddings. Come cerchi velocemente?",
        "",
        "<strong>Soluzione naive:</strong> Calcola cosine similarity con tutti → O(n) = lento",
        "",
        "<strong>Vector Database:</strong> Strutture dati ottimizzate per similarity search",
        "• ANN (Approximate Nearest Neighbor): trova i k più simili in O(log n)",
        "• Indexing: HNSW, IVF, LSH",
        "• Scalabilità: miliardi di vettori, query in millisecondi"
      ],
      "ironicClosing": "PostgreSQL con array? Funziona fino a 100K vettori. Poi serve un vector DB vero."
    },
    {
      "type": "text",
      "title": "Vector DB: Architettura",
      "paragraphs": [
        "<strong>Componenti chiave:</strong>",
        "",
        "<strong>1. Indexing:</strong> HNSW (Hierarchical Navigable Small World) - il gold standard",
        "<strong>2. Storage:</strong> In-memory (veloce) vs On-disk (economico)",
        "<strong>3. Filtering:</strong> Metadata + vector search (es. \"documenti 2025 simili a X\")",
        "<strong>4. Sharding:</strong> Distribuzione su nodi per scalare",
        "<strong>5. Replication:</strong> High availability"
      ],
      "ironicClosing": "HNSW: quando il grafo navigabile ti salva dalle O(n) queries"
    },
    {
      "type": "comparison",
      "title": "Vector DB: Deployment Models",
      "leftSide": {
        "title": "Open Source (Self-hosted)",
        "items": [
          "FAISS (Meta): Libreria, non DB - velocissimo con GPU",
          "ChromaDB: Developer-friendly, <1M vectors",
          "Milvus: Scalabile, enterprise, billions vectors",
          "Qdrant: Rust, performance, filtering avanzato",
          "Weaviate: Hybrid search, GraphQL"
        ]
      },
      "rightSide": {
        "title": "Managed (Cloud)",
        "items": [
          "Pinecone: Fully-managed, veloce, costoso",
          "Weaviate Cloud: Serverless, $25/month start",
          "Qdrant Cloud: Free tier, pay-per-use",
          "AWS OpenSearch Serverless: Vector engine integrato",
          "Azure AI Search: Vector search built-in"
        ]
      },
      "ironicClosing": "Self-hosted = controllo. Managed = sonno tranquillo."
    },
    {
      "type": "data",
      "title": "Vector DB Comparison: Performance",
      "intro": "<strong>Benchmark 2025: Speed matters</strong>",
      "metrics": [
        {
          "value": "50K ops/s",
          "label": "Pinecone - Insert speed"
        },
        {
          "value": "45K ops/s",
          "label": "Qdrant - Insert speed"
        },
        {
          "value": "35K ops/s",
          "label": "Weaviate - Insert speed"
        },
        {
          "value": "5K queries/s",
          "label": "Pinecone - Query speed"
        }
      ],
      "paragraphs": [
        "<strong>FAISS:</strong> Più veloce di tutti (GPU), ma non è un database",
        "<strong>Milvus:</strong> Scala a miliardi, ma setup complesso",
        "<strong>ChromaDB:</strong> Perfetto per prototipi, limite 1M vectors"
      ],
      "ironicClosing": "Velocità costa. Pinecone vince, ma il prezzo è salato.",
      "citations": [
        {
          "text": "Pinecone 50K ops/s insert, Qdrant 45K, Weaviate 35K - benchmark 2025",
          "source": "Xenoss - Vector DB Comparison",
          "url": "https://xenoss.io/blog/vector-database-comparison-pinecone-qdrant-weaviate"
        }
      ]
    },
    {
      "type": "data",
      "title": "Vector DB: Pricing Comparison Nov 2025",
      "intro": "<strong>Quanto costa storage e query?</strong>",
      "metrics": [
        {
          "value": "$0.025/GB/h",
          "label": "Pinecone - Storage ($18/GB/mese)"
        },
        {
          "value": "$2/M queries",
          "label": "Pinecone - Query cost"
        },
        {
          "value": "$102/mese",
          "label": "Qdrant Cloud - Standard tier (AWS us-east)"
        },
        {
          "value": "$25-153",
          "label": "Weaviate Serverless (con/senza compression)"
        }
      ],
      "paragraphs": [
        "<strong>Free tiers:</strong> Qdrant (1GB), ChromaDB (self-hosted), FAISS (self-hosted)",
        "<strong>Enterprise:</strong> Milvus self-hosted + infra cost"
      ],
      "ironicClosing": "Vector storage: il costo nascosto del RAG. 100GB? $1800/mese su Pinecone.",
      "citations": [
        {
          "text": "Pinecone $0.025/GB/hour storage, Qdrant $102/month standard, Weaviate $25-153",
          "source": "Xenoss Vector DB Pricing 2025",
          "url": "https://xenoss.io/blog/vector-database-comparison-pinecone-qdrant-weaviate"
        }
      ]
    },
    {
      "type": "text",
      "title": "Quando Usare Quale Vector DB",
      "paragraphs": [
        "<strong>FAISS:</strong> Ricerca veloce in-memory, no persistence, academic/prototyping",
        "<strong>ChromaDB:</strong> Prototyping, <1M vectors, single node",
        "<strong>Qdrant:</strong> Production, filtering complesso, Rust performance",
        "<strong>Milvus:</strong> Enterprise scale (billions), cloud-native, RBAC",
        "<strong>Pinecone:</strong> Managed, zero-ops, massima velocità (se budget ok)",
        "<strong>Weaviate:</strong> Hybrid search (keyword + vector), GraphQL, multimodal"
      ],
      "ironicClosing": "Prototipo? ChromaDB. Production? Qdrant o Pinecone. Billions? Milvus.",
      "citations": [
        {
          "text": "ChromaDB up to 1M vectors, Milvus billions to trillions scale",
          "source": "Zilliz - Milvus vs ChromaDB",
          "url": "https://zilliz.com/blog/milvus-vs-chroma"
        }
      ]
    },
    {
      "type": "text",
      "title": "RAG: Retrieval Augmented Generation",
      "paragraphs": [
        "<strong>Problema:</strong> LLM hanno knowledge cutoff, non conoscono i tuoi dati privati",
        "",
        "<strong>Soluzione RAG:</strong> Arricchisci il prompt con contesto rilevante",
        "",
        "<strong>Pipeline:</strong>",
        "1. User query → Embedding",
        "2. Vector search → Documenti rilevanti",
        "3. Context + Query → LLM",
        "4. LLM → Risposta basata su dati reali"
      ],
      "ironicClosing": "RAG = dare al LLM una libreria privata invece di inventarsi tutto"
    },
    {
      "type": "code",
      "title": "RAG: Architettura Base",
      "code": {
        "language": "python",
        "snippet": "# RAG Pipeline completa\nfrom openai import OpenAI\nimport chromadb\n\nclient = OpenAI()\nchroma_client = chromadb.Client()\n\n# 1. Setup: Indicizza documenti\ncollection = chroma_client.create_collection(\"docs\")\n\ndocuments = [\n    \"Python è usato per data science\",\n    \"JavaScript è per sviluppo web\",\n    \"Rust è veloce e safe\"\n]\n\ncollection.add(\n    documents=documents,\n    ids=[\"doc1\", \"doc2\", \"doc3\"]\n)\n\n# 2. Query: Ricerca + Generazione\nquery = \"Che linguaggio uso per ML?\"\n\n# Retrieve: Vector search\nresults = collection.query(\n    query_texts=[query],\n    n_results=2\n)\n\ncontext = \"\\n\".join(results['documents'][0])\n\n# Augment + Generate\nresponse = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[{\n        \"role\": \"system\",\n        \"content\": f\"Rispondi basandoti su: {context}\"\n    }, {\n        \"role\": \"user\",\n        \"content\": query\n    }]\n)\n\nprint(response.choices[0].message.content)\n# Output: \"Python è ottimo per ML e data science\""
      },
      "explanation": "Retrieve (vector search) → Augment (context) → Generate (LLM)"
    },
    {
      "type": "text",
      "title": "RAG: Chunking Strategy",
      "paragraphs": [
        "<strong>Problema:</strong> Documenti lunghi non entrano in un embedding. Come spezzarli?",
        "",
        "<strong>Strategie:</strong>",
        "• <strong>Fixed-size:</strong> 512 tokens, overlap 50 (semplice ma grezzo)",
        "• <strong>Sentence-based:</strong> Split per frasi (preserva semantica)",
        "• <strong>Semantic:</strong> Split quando cambia topic (smart ma complesso)",
        "• <strong>Recursive:</strong> Paragrafo → Frase → Token (LangChain default)",
        "",
        "<strong>Best practice:</strong> Chunk size = 256-512 tokens, overlap 10-20%"
      ],
      "ironicClosing": "Chunking sbagliato = contesto perso = risposte sbagliate. Occhio."
    },
    {
      "type": "comparison",
      "title": "RAG Framework: LangChain vs LlamaIndex",
      "leftSide": {
        "title": "LangChain",
        "items": [
          "Orchestration-first: chains, agents, tools",
          "Flessibile: funziona per tutto, non solo RAG",
          "LangSmith: Observability production-ready",
          "Ecosistema maturo: 700+ integrazioni",
          "Curva apprendimento: media-alta"
        ]
      },
      "rightSide": {
        "title": "LlamaIndex",
        "items": [
          "RAG-first: index, query, retrieve",
          "40% più veloce su document retrieval",
          "Query planning, reranking built-in",
          "Data connectors per enterprise (SharePoint, etc)",
          "Curva apprendimento: bassa"
        ]
      },
      "note": "<strong>Consiglio:</strong> LlamaIndex per RAG puro, LangChain per agent complessi con RAG component",
      "ironicClosing": "LlamaIndex fa RAG meglio. LangChain fa tutto. Combinarli? Spesso la scelta migliore.",
      "citations": [
        {
          "text": "LlamaIndex 40% faster document retrieval than LangChain",
          "source": "Latenode - LangChain vs LlamaIndex 2025",
          "url": "https://latenode.com/blog/langchain-vs-llamaindex-2025-complete-rag-framework-comparison"
        }
      ]
    },
    {
      "type": "text",
      "title": "RAG: Agentic RAG",
      "paragraphs": [
        "<strong>RAG classico:</strong> Query → Vector search → LLM (fisso)",
        "",
        "<strong>Agentic RAG:</strong> LLM <em>decide</em> come e dove cercare",
        "",
        "<strong>Capacità:</strong>",
        "• Multi-index routing (quale database interrogare?)",
        "• Query planning (scompone query complessa)",
        "• Reranking (riordina risultati per rilevanza)",
        "• Self-correction (se risposta insufficiente, cerca ancora)"
      ],
      "ironicClosing": "Agentic RAG: quando il retrieval diventa intelligente quanto la generation"
    },
    {
      "type": "text",
      "title": "RAG Hyperscaler: AWS Bedrock",
      "paragraphs": [
        "<strong>Knowledge Bases for Bedrock:</strong> RAG managed end-to-end",
        "",
        "<strong>Features:</strong>",
        "• Auto-chunking, embedding (Titan Embeddings), storage (OpenSearch)",
        "• Multi-model support: Claude, Llama, Mistral",
        "• Agents for Bedrock: action groups, guardrails, traces",
        "",
        "<strong>Best for:</strong> Già su AWS, serve integrazione tight con S3/RDS/Redshift"
      ],
      "ironicClosing": "AWS fa tutto. Anche RAG. Se già paghi AWS, perché no?",
      "citations": [
        {
          "text": "AWS Bedrock Knowledge Bases with auto-chunking, Titan embeddings, OpenSearch",
          "source": "Medium - RAG on AWS, Azure, GCP",
          "url": "https://medium.com/@cloudherowithai/rag-based-architecture-of-three-major-public-clouds-aws-azure-and-gcp-e2cf362fd1e0"
        }
      ]
    },
    {
      "type": "text",
      "title": "RAG Hyperscaler: Azure AI (Foundry)",
      "paragraphs": [
        "<strong>Azure OpenAI Service + Azure AI Search:</strong> RAG con GPT-4",
        "",
        "<strong>Features:</strong>",
        "• Azure AI Search: Vector search built-in, hybrid (keyword + vector)",
        "• Prompt Flow: Visual RAG pipeline builder",
        "• Content Safety: Guardrails automatici",
        "",
        "<strong>Best for:</strong> Enterprise Microsoft-centric, compliance stretta, GPT-4 integration"
      ],
      "ironicClosing": "Azure AI: quando vuoi GPT-4 enterprise con compliance Microsoft-grade"
    },
    {
      "type": "text",
      "title": "RAG Hyperscaler: Google Vertex AI",
      "paragraphs": [
        "<strong>Vertex AI Search + Agent Builder:</strong> RAG multimodal",
        "",
        "<strong>Features:</strong>",
        "• Vertex AI Vector Search: Scalabile, low-latency",
        "• Agent Builder: No-code + LangChain/LlamaIndex integration",
        "• Multimodal: Text + Image search",
        "",
        "<strong>Best for:</strong> Data-heavy orgs, BigQuery integration, multimodal use cases"
      ],
      "ironicClosing": "Google Vertex: quando i tuoi dati sono già su BigQuery e GCS",
      "citations": [
        {
          "text": "Vertex AI Agent Builder with no-code and LangChain/LlamaIndex integration",
          "source": "Medium - Vertex AI Guide 2025",
          "url": "https://blog.gopenai.com/azure-ai-foundry-vs-aws-bedrock-vs-google-vertex-ai-the-2025-guide-25a69c1d19b1"
        }
      ]
    },
    {
      "type": "text",
      "title": "OpenAI Vector Store & File Search",
      "paragraphs": [
        "<strong>Novità 2024-2025:</strong> OpenAI Assistants API v2 con Vector Store built-in",
        "",
        "<strong>Come funziona:</strong>",
        "1. Upload file (PDF, DOCX, TXT, code)",
        "2. Auto-chunking, embedding, vector storage (managed)",
        "3. File Search tool: RAG automatico nelle chat",
        "",
        "<strong>Pricing (Nov 2025):</strong>",
        "• Storage: $0.10/GB/day (primo 1GB free)",
        "• Query: token cost standard (input + output)",
        "",
        "<strong>Limiti:</strong> Max 10,000 files per vector store, 512MB per file"
      ],
      "ironicClosing": "OpenAI fa RAG in 3 righe di codice. Magia? No, Assistants API.",
      "citations": [
        {
          "text": "OpenAI Vector Store $0.10/GB/day storage, first 1GB free",
          "source": "OpenAI Community - File Search Pricing",
          "url": "https://community.openai.com/t/how-file-search-works-and-pricing/805817"
        }
      ]
    },
    {
      "type": "code",
      "title": "OpenAI Vector Store: Esempio",
      "code": {
        "language": "python",
        "snippet": "from openai import OpenAI\n\nclient = OpenAI()\n\n# 1. Create vector store\nvector_store = client.beta.vector_stores.create(\n    name=\"Company Knowledge Base\"\n)\n\n# 2. Upload files\nfile_paths = [\"policy.pdf\", \"handbook.docx\"]\nfile_streams = [open(path, \"rb\") for path in file_paths]\n\nfile_batch = client.beta.vector_stores.file_batches.upload_and_poll(\n    vector_store_id=vector_store.id,\n    files=file_streams\n)\n\n# 3. Create assistant with file search\nassistant = client.beta.assistants.create(\n    name=\"HR Assistant\",\n    instructions=\"You answer questions about company policies.\",\n    model=\"gpt-4-turbo\",\n    tools=[{\"type\": \"file_search\"}],\n    tool_resources={\n        \"file_search\": {\n            \"vector_store_ids\": [vector_store.id]\n        }\n    }\n)\n\n# 4. Ask question (RAG automatico)\nthread = client.beta.threads.create()\nclient.beta.threads.messages.create(\n    thread_id=thread.id,\n    role=\"user\",\n    content=\"What's the vacation policy?\"\n)\n\nrun = client.beta.threads.runs.create_and_poll(\n    thread_id=thread.id,\n    assistant_id=assistant.id\n)\n\nmessages = client.beta.threads.messages.list(thread_id=thread.id)\nprint(messages.data[0].content[0].text.value)"
      },
      "explanation": "RAG completamente managed: upload, embed, search, answer - zero setup"
    },
    {
      "type": "data",
      "title": "RAG Production: Costi Reali (Nov 2025)",
      "intro": "<strong>Esempio: 100K documenti, 10K queries/giorno</strong>",
      "metrics": [
        {
          "value": "~50GB",
          "label": "Vector storage (embedding 1536 dim)"
        },
        {
          "value": "$900/mese",
          "label": "Pinecone storage cost"
        },
        {
          "value": "$150/mese",
          "label": "OpenAI Vector Store (50GB)"
        },
        {
          "value": "$15-50",
          "label": "Embedding cost (one-time, 100K docs)"
        }
      ],
      "paragraphs": [
        "<strong>Query cost:</strong> Dipende da LLM (GPT-4: ~$0.01-0.03/query con context)",
        "<strong>Self-hosted:</strong> Qdrant/Milvus + GPU instance = $200-500/mese",
        "<strong>Hidden costs:</strong> Reindexing, monitoring, reranking"
      ],
      "ironicClosing": "Storage è il costo fisso. Queries il variabile. Monitora entrambi.",
      "citations": [
        {
          "text": "Pinecone $0.025/GB/hour = $18/GB/month storage cost",
          "source": "Xenoss - Vector DB Pricing",
          "url": "https://xenoss.io/blog/vector-database-comparison-pinecone-qdrant-weaviate"
        }
      ]
    },
    {
      "type": "text",
      "title": "RAG Best Practices 2025",
      "paragraphs": [
        "<strong>1. Chunking:</strong> 256-512 tokens, overlap 10-20%, preserve semantica",
        "<strong>2. Metadata filtering:</strong> Date, source, type - filtra prima di search",
        "<strong>3. Reranking:</strong> Cohere Rerank API ($1/1K searches) migliora recall",
        "<strong>4. Hybrid search:</strong> Vector + keyword per best results",
        "<strong>5. Monitoring:</strong> Track retrieval quality, latency, costs",
        "<strong>6. Caching:</strong> Cache queries frequenti (risparmi 90% LLM cost)",
        "<strong>7. Evaluation:</strong> RAGAS, TruLens per misurare RAG quality"
      ],
      "ironicClosing": "RAG senza monitoring è come guidare bendati. Funziona finché non funziona."
    },
    {
      "type": "summary",
      "title": "Riepilogo: Embeddings, Vector DB, RAG",
      "items": [
        "Embeddings: Testo → Vettori che capiscono significato (1536-3072 dim)",
        "Similarity: Cosine similarity misura vicinanza semantica",
        "Modelli: OpenAI $0.02-0.13/1M, Cohere $0.50/1M, Sentence Transformers free",
        "Vector DB: FAISS (veloce), ChromaDB (proto), Qdrant/Milvus (production), Pinecone (managed)",
        "Pricing: Pinecone $18/GB/mese, Qdrant $102, OpenAI $0.10/GB/day",
        "RAG: Retrieve + Augment + Generate - LLM con knowledge base privata",
        "Framework: LlamaIndex (RAG-first), LangChain (flexible)",
        "Hyperscaler: AWS Bedrock, Azure AI, Google Vertex - RAG managed",
        "OpenAI Vector Store: RAG in 3 righe, $0.10/GB/day, primo GB free"
      ],
      "ironicClosing": "Embeddings + Vector DB + RAG = Il trio che fa funzionare l'AI su dati reali. Fine 2025: production-ready."
    }
  ],
  "lastTranslated": null,
  "sourceLanguage": "it"
}